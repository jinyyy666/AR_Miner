{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: App-Review Miner\n",
    "Team members: Shanshan Li, Yingyezhe Jin, Tianshu Chu, Xiao Huang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put all import here\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP based preprocessing\n",
    "\n",
    "    Inputs:  datasetName\n",
    "             rmStopWords control to remove stop words\n",
    "             rmRareWords control to remove rarely occured words\n",
    "             \n",
    "    Outputs: trainSet    is a list of training reviews\n",
    "             testSet     is a list of testing reviews\n",
    "             unlabelSet  is a list of unlabeld reviews\n",
    "             vocabulary  is the corresponding n vocabulary in a dictionary form {word, index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for templerun2 : 5527\n",
      "Training set Size: 1000\n",
      "Testing set Size: 2000\n",
      "Unlabeling set Size: 57559\n"
     ]
    }
   ],
   "source": [
    "%run ./AR_util.py\n",
    "%run ./AR_reviewInstance.py\n",
    "datasetName = \"templerun2\" # four apps : facebook, templerun2, swiftkey, tapfish\n",
    "\n",
    "rmStopWords = False # Removing stop words lead to information loss and bad f-score\n",
    "rmRareWords = True\n",
    "\n",
    "# trainSet/testSet/unlabel: dictionary of {label, reviews} for review data\n",
    "# vocabulary: dictionary len = V and the positional index of each term in the doc vector\n",
    "# set skParse True to directly read of the data that has been filtered out\n",
    "skParse = False\n",
    "if(skParse == False):\n",
    "    # the vocabulary is the words on the training set!\n",
    "    trainSet, testSet, unlabelSet, vocabulary = AR_parse(datasetName, rmStopWords, rmRareWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes based filtering\n",
    "\n",
    "    Inputs:  train/test/unlabelSet   are the preprocessed reviews \n",
    "             vocabulary              is the corresponding vocabulary of the reviews\n",
    "    Outputs: informMat    is a k*m np sparse matrix, k is the number of informative reviews, m is the length of vocabulary\n",
    "             informRev    is a list of informative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F-Score for the test data: 0.837037037037\n",
      "Number of informative reviews: 14684\n"
     ]
    }
   ],
   "source": [
    "%run ./AR_classifier.py\n",
    "# 1. Use the EM-NB or SVM to filter out the informative reviews\n",
    "# informMat: the informative reviews in X x V sparse matrix from, X: documents size, V: vocabulary size\n",
    "# informRev: corresponding reviews wrapped as a list of review instances\n",
    "useSVM = True # SVM is way better than emnb in terms of the testing. \n",
    "               # But it may not filter out the information effectively\n",
    "if(skParse == False):\n",
    "    if(useSVM == False):\n",
    "        informRev, informMat = AR_emnb(trainSet, testSet, unlabelSet, vocabulary, datasetName)\n",
    "    else:\n",
    "        informRev, informMat = AR_svm(trainSet, testSet, unlabelSet, vocabulary, datasetName)\n",
    "\n",
    "    # write the result back to the file (optional)\n",
    "    AR_writeReviews(informRev, datasetName)\n",
    "else:\n",
    "    # directly read from the file\n",
    "    informRev, informMat, vocabulary = AR_loadReviews(datasetName)\n",
    "\n",
    "print(\"Number of informative reviews: \" + str(len(informRev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LDA topic clustering\n",
    "\n",
    "    Inputs:  informMat  is the k*m np sparse matrix, k is the number of informative reviews, m  is the length of vocabulary\n",
    "             informRev  is the informative review list\n",
    "             vocabulary is the corresponding vocabulary dictionary\n",
    "             n_topics   is the number of topics\n",
    "    Outputs: doc_topic  is a k*n_topics np matrix, which indicates the probability\n",
    "             vocab      is the vocabulary in the list form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 14684\n",
      "INFO:lda:vocab_size: 5527\n",
      "INFO:lda:n_words: 238165\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -2449521\n",
      "INFO:lda:<500> log likelihood: -1530662\n",
      "INFO:lda:<1000> log likelihood: -1530186\n",
      "INFO:lda:<1499> log likelihood: -1531980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: it i to close and download time\n",
      "Topic 1: it but game a lag is sometim\n",
      "Topic 2: it i me play t game my\n",
      "Topic 3: i and coin the for have but\n",
      "Topic 4: run the templ but first i it\n",
      "Topic 5: the i when game and me save\n",
      "Topic 6: on my it phone galaxi not work\n",
      "Topic 7: it star i fix 5 thi but\n",
      "Topic 8: you to have if that a can\n",
      "Topic 9: the up power you a and when\n",
      "Topic 10: the is that onli a of problem\n",
      "Topic 11: the updat on fix game lag it\n",
      "Topic 12: the it screen to and then i\n",
      "Topic 13: coin object the i 000 have but\n",
      "Topic 14: i coin my and to all game\n",
      "Topic 15: the and but a more game new\n",
      "Topic 16: t i can it doesn the don\n",
      "Topic 17: the to i and when turn jump\n",
      "Topic 18: a it to but time take game\n",
      "Topic 19: to the sensit tilt need be is\n"
     ]
    }
   ],
   "source": [
    "%run ./AR_lda.py\n",
    "# 2. Use the LDA to do the grouping based on the topic\n",
    "# doc_topi : a k*n_topics np matrix, which indicates the probability of each review belongs to one of the topic\n",
    "# vocab: a list of vocabulary words\n",
    "n_topics = 20\n",
    "doc_topic, vocab = AR_lda(informRev, informMat, vocabulary, n_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 13, 14, 8, 14, 14, 14, 14, 14, 8, 8, 13, 14, 14, 8, 8, 8, 8, 6, 17, 17, 19, 11, 16, 4, 4, 8, 8, 12, 8, 8, 1, 4, 8, 8, 8, 8, 6, 6, 13, 14, 13, 4, 13, 17, 1, 1, 17, 17, 3, 3, 17, 4, 16, 13, 9, 9, 13, 13, 1, 17, 11, 1, 8, 10, 4, 4, 10, 18, 4, 4, 17, 4, 12, 4, 4, 4, 16, 16, 11, 11, 11, 11, 13, 10, 4, 11, 4, 12, 12, 13, 10, 0, 0, 13, 4, 4, 1, 0, 12, 4, 10, 5, 13, 10, 4, 4, 4, 1, 3, 3, 3, 19, 1, 3, 3, 3, 5, 4, 17, 17, 17, 17, 17, 17, 13, 11, 14, 13, 17, 0, 6, 14, 14, 15, 6, 13, 2, 3, 17, 8, 9, 1, 14, 5, 11, 15, 15, 15, 3, 4, 11, 8, 6, 1, 15, 19, 3, 9, 5, 13, 11, 13, 16, 13, 0, 13, 0, 0, 14, 8, 8, 3, 3, 1, 3, 10, 10, 8, 13, 10, 13, 5, 8, 10, 13, 8, 13, 8, 6, 13, 2, 5, 5, 6, 9, 6, 11, 1, 18, 10, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 18, 18, 18, 18, 16, 16, 18, 13, 18, 5, 10, 13, 1, 12, 18, 18, 9, 10, 9, 3, 13, 2, 11, 7, 7, 9, 9, 10, 9, 13, 3, 19, 19, 3, 1, 13, 7, 9, 6, 7, 7, 1, 8, 7, 3, 0, 16, 7, 7, 13, 1, 7, 1, 11, 9, 6, 6, 16, 0, 13, 11, 13, 1, 19, 13, 3, 1, 19, 4, 13, 9, 8, 1, 12, 6, 5, 3, 13, 13, 19, 13, 11, 10, 10, 13, 5, 2, 12, 12, 13, 13, 6, 19, 1, 1, 13, 6, 13, 3, 16, 6, 13, 0, 13, 10, 0, 13, 16, 0, 14, 13, 19, 5, 13, 4, 8, 1, 1, 1, 1, 4, 16, 4, 4, 4, 13, 1, 5, 16, 0, 3, 13, 13, 13, 11, 13, 13, 17, 1, 8, 6, 4, 8, 6, 19, 4, 0, 8, 10, 0, 5, 5, 16, 5, 5, 5, 18, 5, 8, 14, 15, 3, 5, 1, 8, 8, 13, 5, 13, 5, 8, 5, 8, 16, 13, 2, 2, 2, 17, 16, 2, 16, 6]\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "ClustNum = []\n",
    "# print doc_topic[1]\n",
    "for i in range(k):\n",
    "    ClustNum.append(doc_topic[i].argmax())\n",
    "print ClustNum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ranking all the groups based on importance\n",
    "\n",
    "    Inputs:  \n",
    "    Outputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ranking all the reviews in each group based on text rank\n",
    "\n",
    "    Inputs:  \n",
    "    Outputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
